# -*- coding: utf-8 -*-
"""creditcardfraud

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nGTKCAMvuhnCJcIwzkqPQpCUqsz8-Q1G
"""

# Install necessary libraries
!pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn joblib streamlit pyngrok

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Mount Google Drive (if your dataset is stored there)
from google.colab import drive
drive.mount('/content/drive')

# Load dataset (upload creditcard.csv first or use from Kaggle)
df = pd.read_csv('/content/creditcard.csv')

# Basic exploration
print(f"Dataset shape: {df.shape}")
print("\nFirst 5 rows:")
print(df.head())

# Check for missing values
print("\nMissing values per column:")
print(df.isnull().sum())

# Class distribution
print("\nClass distribution:")
print(df['Class'].value_counts(normalize=True))

# Visualize class imbalance
plt.figure(figsize=(8,6))
sns.countplot(x='Class', data=df)
plt.title('Class Distribution (0=Genuine, 1=Fraud)')
plt.show()

import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('creditcard.csv')

# 1. Check initial NaN counts
print("Initial NaN in features:", df.drop('Class', axis=1).isna().sum().sum())
print("Initial NaN in target:", df['Class'].isna().sum())

# 2. Remove ALL rows with ANY NaN values (more aggressive cleaning)
df = df.dropna(how='any')  # This removes rows with NaN in any column

# 3. Verify complete cleaning
print("\nAfter cleaning:")
print("NaN in features:", df.drop('Class', axis=1).isna().sum().sum())
print("NaN in target:", df['Class'].isna().sum())

# 4. Check data types
print("\nData types:")
print(df.dtypes)

from sklearn.preprocessing import StandardScaler

# 1. Create time-based feature
df['Hour'] = df['Time'].apply(lambda x: x / 3600 % 24)

# 2. Scale features - IMPORTANT: Use the same scaler instance
scaler = StandardScaler()
scale_cols = ['Amount', 'Hour']
df[['Amount_scaled', 'Hour_scaled']] = scaler.fit_transform(df[scale_cols])

# 3. Final feature selection
features = [f'V{i}' for i in range(1, 29)] + ['Amount_scaled', 'Hour_scaled']
X = df[features]
y = df['Class']

# Final verification
assert y.isna().sum() == 0, "Target still contains NaN values!"
assert X.isna().sum().sum() == 0, "Features still contain NaN values!"

from sklearn.model_selection import train_test_split

# 1. Split with additional checks
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )

    print("Split successful!")
    print(f"Train shapes: X={X_train.shape}, y={y_train.shape}")
    print(f"Test shapes: X={X_test.shape}, y={y_test.shape}")

except ValueError as e:
    print("Error during split:", e)
    print("\nDebugging info:")
    print("NaN in X:", X.isna().sum().sum())
    print("NaN in y:", y.isna().sum())
    print("Unique y values:", y.unique())

from imblearn.over_sampling import SMOTE

# 1. Final NaN check
print("Final check before SMOTE:")
print("NaN in X_train:", X_train.isna().sum().sum())
print("NaN in y_train:", y_train.isna().sum())

# 2. Apply SMOTE
if y_train.isna().sum() == 0:
    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
    print("\nSMOTE successful!")
    print("Resampled class distribution:", pd.Series(y_train_smote).value_counts())
else:
    print("\nERROR: Cannot apply SMOTE - NaN values detected in y_train")
    print("Debugging steps:")
    print("1. Check data loading")
    print("2. Verify dropna() was used")
    print("3. Inspect y_train:", y_train.head())

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Initialize and train model
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    class_weight='balanced_subsample',
    random_state=42
)

model.fit(X_train_smote, y_train_smote)

# Generate predictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # Fraud probabilities

# Evaluate performance
print("Classification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print(f"\nROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}")

from sklearn.metrics import precision_recall_curve

# Find optimal threshold based on F1-score
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal threshold: {optimal_threshold:.4f}")

# Apply optimal threshold
y_pred_optimal = (y_proba >= optimal_threshold).astype(int)

print("\nPerformance with Optimal Threshold:")
print(classification_report(y_test, y_pred_optimal))

# Get feature importances
importances = model.feature_importances_
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importances
}).sort_values('Importance', ascending=False)

# Plot top 20 features
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))
plt.title('Top 20 Important Features for Fraud Detection')
plt.show()

import joblib

# Save model and scaler
joblib.dump(model, 'fraud_detection_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(optimal_threshold, 'optimal_threshold.pkl')

# Download files locally (in Colab)
from google.colab import files
files.download('fraud_detection_model.pkl')
files.download('scaler.pkl')
files.download('optimal_threshold.pkl')